{"cells":[{"cell_type":"code","execution_count":1,"id":"d40d05c8","metadata":{},"outputs":[{"data":{"text/plain":["sparkSession = org.apache.spark.sql.SparkSession@48e4e607\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["org.apache.spark.sql.SparkSession@48e4e607"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["val sparkSession=new SparkSession.Builder().appName(\"broadCastingSession\").getOrCreate()"]},{"cell_type":"code","execution_count":15,"id":"5c6a46fb","metadata":{},"outputs":[{"data":{"text/plain":["generateUserData = > Unit = $Lambda$5007/0x0000000101dff040@35cca140\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["> Unit = $Lambda$5007/0x0000000101dff040@35cca140"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["\n","import org.apache.spark.sql.{SparkSession, Row}          // For Spark Session and Row\n","import scala.util.{Random}\n","import org.apache.spark.sql.types._\n","val generateUserData:()=>Unit=()=>{\n","    \n","    val numRecords = 10000\n","    val random = new Random()\n","\n","    // Generate user data (userId and Name)\n","    val data = (1 to numRecords).map { i =>\n","      val userId = i\n","      val name = s\"User-${i}\"\n","      Row(userId, name)\n","    }\n","    \n","    \n","   val schema = StructType(Array(\n","      StructField(\"userId\", IntegerType, nullable = false),\n","      StructField(\"Name\", StringType, nullable = false)\n","    ))\n","    val rdd = sparkSession.sparkContext.parallelize(data)\n","    val df = sparkSession.createDataFrame(rdd, schema)\n","\n","    // Show a few records\n","    df.show(10)\n","\n","    // Set GCS bucket path\n","    val gcsPath = \"gs://second_task/user_data.csv\"  // Change this to your GCS path\n","\n","    // Save DataFrame to GCS as CSV (you can also save as Parquet, JSON, etc.)\n","    df.write\n","      .format(\"csv\")\n","      .option(\"header\", \"true\")\n","      .mode(\"overwrite\")  // Options: \"overwrite\", \"append\"\n","      .save(gcsPath)\n","\n","    println(s\"Dataset saved to GCS path: $gcsPath\")\n","\n","    // Stop the Spark session\n","//     sparkSession.stop()\n","}"]},{"cell_type":"code","execution_count":16,"id":"b37fc362","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+------+-------+\n","|userId|   Name|\n","+------+-------+\n","|     1| User-1|\n","|     2| User-2|\n","|     3| User-3|\n","|     4| User-4|\n","|     5| User-5|\n","|     6| User-6|\n","|     7| User-7|\n","|     8| User-8|\n","|     9| User-9|\n","|    10|User-10|\n","+------+-------+\n","only showing top 10 rows\n","\n","Dataset saved to GCS path: gs://second_task/user_data.csv\n"]}],"source":["generateUserData()"]},{"cell_type":"code","execution_count":18,"id":"ff8dc60a","metadata":{},"outputs":[{"data":{"text/plain":["generateUserTransactionData = > Unit = $Lambda$5277/0x0000000100288840@6dae486b\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["> Unit = $Lambda$5277/0x0000000100288840@6dae486b"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["val generateUserTransactionData:()=>Unit=()=>{\n","    \n","    val numRecords=10000000\n","    \n","//     val data = (1 to numRecords).map { i =>\n","//       val transactionId=i\n","//       val userId =  random.nextInt(10000) + 1\n","//       val transaction = s\"transaction-${i}\"\n","//       Row(transactionId,userId, transaction)\n","//     }\n","    val random=new Random()\n","    \n","   val schema = StructType(Array(\n","       StructField(\"transactionId\",IntegerType,nullable=false),\n","      StructField(\"userId\", IntegerType, nullable = false),\n","      StructField(\"transaction\", StringType, nullable = false)\n","    ))\n","//     val rdd = sparkSession.sparkContext.parallelize(data)\n","    //directly parallelizing the data creation\n","    val dataRDD = sparkSession.sparkContext.parallelize(1 to numRecords, numSlices = 100).map { i =>\n","        val transactionId = i\n","        val userId = random.nextInt(10000) + 1  // Random userId between 1 and 10000\n","        val transaction = s\"transaction-${i}\"   // Transaction string\n","        Row(transactionId, userId, transaction)\n","  }\n","\n","    val df = sparkSession.createDataFrame(dataRDD, schema)\n","\n","    // Show a few records\n","    df.show(10)\n","\n","    // Set GCS bucket path\n","    val gcsPath = \"gs://second_task/transaction_data.csv\"  // Change this to your GCS path\n","\n","    // Save DataFrame to GCS as CSV (you can also save as Parquet, JSON, etc.)\n","    df.write\n","      .format(\"csv\")\n","      .option(\"header\", \"true\")\n","      .mode(\"overwrite\")  // Options: \"overwrite\", \"append\"\n","      .save(gcsPath)\n","\n","    println(s\"Dataset saved to GCS path: $gcsPath\")\n","    \n","}"]},{"cell_type":"code","execution_count":19,"id":"ac8bfe5a","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+-------------+------+--------------+\n","|transactionId|userId|   transaction|\n","+-------------+------+--------------+\n","|            1|  5871| transaction-1|\n","|            2|  7933| transaction-2|\n","|            3|   719| transaction-3|\n","|            4|  6511| transaction-4|\n","|            5|  6172| transaction-5|\n","|            6|  8141| transaction-6|\n","|            7|  4128| transaction-7|\n","|            8|  3657| transaction-8|\n","|            9|  1008| transaction-9|\n","|           10|  5235|transaction-10|\n","+-------------+------+--------------+\n","only showing top 10 rows\n","\n","Dataset saved to GCS path: gs://second_task/transaction_data.csv\n"]}],"source":["generateUserTransactionData()"]},{"cell_type":"code","execution_count":20,"id":"78c78bb8","metadata":{},"outputs":[{"data":{"text/plain":["userDataCSVPath = gs://second_task/user_data.csv\n","userDataDf = [userId: int, Name: string]\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["[userId: int, Name: string]"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["val userDataCSVPath=\"gs://second_task/user_data.csv\"\n","val userDataDf=sparkSession.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(userDataCSVPath)"]},{"cell_type":"code","execution_count":21,"id":"bb1b79d4","metadata":{},"outputs":[{"data":{"text/plain":["transactionDataCSVPath = gs://second_task/transaction_data.csv\n","transactionDataDf = [transactionId: int, userId: int ... 1 more field]\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["[transactionId: int, userId: int ... 1 more field]"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["val transactionDataCSVPath=\"gs://second_task/transaction_data.csv\"\n","val transactionDataDf=sparkSession.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(transactionDataCSVPath)"]},{"cell_type":"code","execution_count":23,"id":"cb2d7384","metadata":{},"outputs":[{"data":{"text/plain":["gs://second_task/joined_transaction_data_broadcasted.csv"]},"execution_count":23,"metadata":{},"output_type":"execute_result"},{"data":{"text/plain":["broadcastedUserDataDf = [userId: int, Name: string]\n","joinedDf = [userId: int, transactionId: int ... 2 more fields]\n","gcsJoinPath = gs://second_task/joined_transaction_data_broadcasted.csv\n"]},"metadata":{},"output_type":"display_data"}],"source":["//spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"50MB\")  // Example to set threshold to 50MB\n","//can set the broadCasting threshold like this\n","import org.apache.spark.sql.functions._  // Importing the broadcast function\n","val broadcastedUserDataDf = broadcast(userDataDf)\n","val joinedDf=transactionDataDf.join(broadcastedUserDataDf,\"userId\")\n","val gcsJoinPath = \"gs://second_task/joined_transaction_data_broadcasted.csv\"\n","joinedDf.write\n","  .format(\"csv\")\n","  .option(\"header\", \"true\")\n","  .mode(\"overwrite\")\n","  .save(gcsJoinPath)"]},{"cell_type":"markdown","id":"42e4c86f","metadata":{},"source":["### Benefits of Broadcasting in This Case\n","\n","- **Improved Performance:**\n","  - Broadcasting allows Spark to avoid shuffling the larger DataFrame (`transactionDataDf` in this case) during the join operation.\n","  - This reduces the amount of data exchanged between worker nodes.\n","  - The reduction in data transfer speeds up the join process significantly.\n","\n","- **Cost-Effective:**\n","  - Broadcasting a small DataFrame ensures it is sent to all nodes only once.\n","  - This makes the process more efficient in terms of:\n","    - **Memory usage**: The DataFrame is stored in memory only once on each node.\n","    - **Computation time**: Avoids repeated reads of the small DataFrame, especially when accessed multiple times.\n"]},{"cell_type":"code","execution_count":null,"id":"1178a1cb","metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Apache Toree - Scala","language":"scala","name":"apache_toree_scala"},"language_info":{"codemirror_mode":"text/x-scala","file_extension":".scala","mimetype":"text/x-scala","name":"scala","pygments_lexer":"scala","version":"2.12.15"}},"nbformat":4,"nbformat_minor":5}