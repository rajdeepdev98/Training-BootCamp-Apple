{"cells":[{"cell_type":"code","execution_count":3,"id":"1e2bff47","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["File transferred from gs://deva_vasadi/movie_lens_data/movies.csv to hdfs:///user/casestudies/casestudy4/movies.csv successfully!\n"]},{"data":{"text/plain":["lastException = null\n","spark = org.apache.spark.sql.SparkSession@2635be6e\n","gcsPath = gs://deva_vasadi/movie_lens_data/movies.csv\n","hdfsPath = hdfs:///user/casestudies/casestudy4/movies.csv\n","data = [movieId: string, title: string ... 1 more field]\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["[movieId: string, title: string ... 1 more field]"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["//Transfer the file from GCS to HDFS\n","\n","import org.apache.spark.sql.SparkSession\n","\n","// Step 1: Initialize SparkSession\n","val spark = SparkSession.builder()\n","  .appName(\"Transfer File from GCS to HDFS\")\n","  .getOrCreate()\n","\n","// Step 2: Define the GCS path and HDFS path\n","val gcsPath = \"gs://deva_vasadi/movie_lens_data/movies.csv\"\n","val hdfsPath = \"hdfs:///user/casestudies/casestudy4/movies.csv\"\n","\n","// Step 3: Read the file from GCS\n","val data = spark.read\n","  .option(\"header\", \"true\")  // Read the header from the CSV file\n","  .csv(gcsPath)\n","\n","// Step 4: Write the file to HDFS with headers\n","data.write\n","  .option(\"header\", \"true\")  // Include the header in the output\n","  .mode(\"overwrite\")         // Overwrite if the file already exists\n","  .csv(hdfsPath)\n","\n","println(s\"File transferred from $gcsPath to $hdfsPath successfully!\")"]},{"cell_type":"code","execution_count":4,"id":"55a8ca5f","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["1000 duplicates inserted and file updated successfully!\n"]},{"data":{"text/plain":["moviesPath = hdfs:///user/casestudies/casestudy4/movies.csv\n","moviesDF = [movieId: string, title: string ... 1 more field]\n","duplicateMoviesDF = [movieId: string, title: string ... 1 more field]\n","moviesWithDuplicatesDF = [movieId: string, title: string ... 1 more field]\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["[movieId: string, title: string ... 1 more field]"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["//Adding duplicates\n","\n","// Step 1: Read the existing movies.csv from HDFS\n","val moviesPath = \"hdfs:///user/casestudies/casestudy4/movies.csv\"\n","val moviesDF = spark.read.option(\"header\", \"true\").csv(moviesPath)\n","\n","// Step 2: Add 1000 duplicates by appending the same DataFrame multiple times\n","val duplicateMoviesDF = moviesDF.limit(1000) // Take 1000 rows to duplicate\n","val moviesWithDuplicatesDF = moviesDF.union(duplicateMoviesDF) // Append duplicates\n","\n","// Step 3: Write the updated DataFrame with duplicates back to the same HDFS path\n","moviesWithDuplicatesDF.write\n","  .option(\"header\", \"true\")\n","  .mode(\"overwrite\") // Overwrite the existing file\n","  .csv(\"hdfs:///user/casestudies/casestudy4/duplicated_movies.csv\")\n","\n","println(\"1000 duplicates inserted and file updated successfully!\")\n"]},{"cell_type":"code","execution_count":12,"id":"4171c1a9","metadata":{"scrolled":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Marking org.apache.spark:spark-avro_2.12:3.3.2 for download\n","Obtained 12 files\n"]},{"data":{"text/plain":["lastException = null\n"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Marking org.apache.spark:spark-avro_2.12:3.3.2 for download\n","Obtained 12 files\n"]}],"source":["// Add depedencies for avro format\n","%AddDeps org.apache.spark spark-avro_2.12 3.3.2 --transitive"]},{"cell_type":"code","execution_count":13,"id":"5a8f8d3f","metadata":{"scrolled":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Running...\n","Original record count: 88586\n","Deduplicated record count: 87585\n","Duplicates removed: 1001\n"]},{"data":{"text/plain":["spark = org.apache.spark.sql.SparkSession@2635be6e\n","moviesPath = hdfs:///user/casestudies/casestudy4/duplicated_movies.csv\n","moviesDF = [movieId: string, title: string ... 1 more field]\n","moviesRDD = MapPartitionsRDD[163] at map at <console>:82\n","uniqueMoviesRDD = ShuffledRDD[164] at reduceByKey at <console>:90\n","cleanedMoviesRDD = MapPartitionsRDD[165] at map at <console>:93\n","cleanedMoviesDF = [movieId: string, title: string ... 1 more field]\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["originalCou...\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["[movieId: string, title: string ... 1 more field]"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["//Remove duplicates and store the final CSV in avro format\n","\n","import org.apache.spark.sql.SparkSession\n","import org.apache.spark.sql.functions._\n","\n","// Step 1: Initialize SparkSession\n","val spark = SparkSession.builder()\n","  .appName(\"Movies Duplicate Cleaner\")\n","  .getOrCreate()\n","\n","// Step 2: Load movies.csv into a DataFrame from HDFS\n","val moviesPath = \"hdfs:///user/casestudies/casestudy4/duplicated_movies.csv\"\n","val moviesDF = spark.read.option(\"header\", \"true\").csv(moviesPath)\n","\n","//moviesDF.show()\n","\n","println(\"Running...\")\n","\n","// Step 3: Convert to RDD with composite key (movieId, title)\n","val moviesRDD = moviesDF.rdd.map(row => {\n","  val movieId = row.getString(row.fieldIndex(\"movieId\"))\n","  val title = row.getString(row.fieldIndex(\"title\"))\n","  val genres = row.getString(row.fieldIndex(\"genres\"))\n","  ((movieId, title), genres) // Key: (movieId, title), Value: genres\n","})\n","\n","// Combine genres for duplicate keys\n","val uniqueMoviesRDD = moviesRDD.reduceByKey((genres1, genres2) => s\"$genres1|$genres2\")\n","\n","// Transform back to (movieId, title, genres) format\n","val cleanedMoviesRDD = uniqueMoviesRDD.map {\n","  case ((movieId, title), combinedGenres) => (movieId, title, combinedGenres)\n","}\n","\n","val cleanedMoviesDF = cleanedMoviesRDD.toDF(\"movieId\", \"title\", \"genres\")\n","\n","// Step 4: Validation\n","val originalCount = moviesDF.count()\n","val deduplicatedCount = cleanedMoviesDF.count()\n","val duplicatesRemoved = originalCount - deduplicatedCount\n","println(s\"Original record count: $originalCount\")\n","println(s\"Deduplicated record count: $deduplicatedCount\")\n","println(s\"Duplicates removed: $duplicatesRemoved\")\n"]},{"cell_type":"code","execution_count":14,"id":"d9c8840b","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Cleaned movies data saved successfully in Avro format.\n"]},{"data":{"text/plain":["outputPath = gs://deva_vasadi/case_studies/cleaned-movies.avro\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["gs://deva_vasadi/case_studies/cleaned-movies.avro"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["// Step 6: Save the cleaned data as Avro files in GCP Cloud Storage\n","val outputPath = \"gs://deva_vasadi/case_studies/cleaned-movies.avro\"\n","cleanedMoviesDF.write\n","  .format(\"avro\")\n","  .mode(\"overwrite\")\n","  .save(outputPath)\n","\n","println(\"Cleaned movies data saved successfully in Avro format.\")"]},{"cell_type":"code","execution_count":6,"id":"c284fd49","metadata":{},"outputs":[{"data":{"text/plain":["countDuplicates: (inputDF: org.apache.spark.sql.DataFrame)Long\n"]},"metadata":{},"output_type":"display_data"}],"source":["//Method to count duplicates\n","\n","import org.apache.spark.sql.DataFrame\n","import org.apache.spark.sql.functions._\n","\n","def countDuplicates(inputDF: DataFrame): Long = {\n","  // Step 1: Group by specified columns and count occurrences\n","  val groupedDF = inputDF\n","    .groupBy(\"movieId\", \"title\")\n","    .count()\n","    \n","  //groupedDF.show()\n","\n","  // Step 2: Filter groups with more than 1 record (duplicates)\n","  val duplicateGroupsDF = groupedDF.filter(col(\"count\") > 1)\n","    \n","  //duplicateGroupsDF.show()\n","  if (duplicateGroupsDF.head(1).isEmpty) {\n","    println(\"No duplicate groups found.\")\n","    return 0L\n","  }\n","\n","  // Step 3: Calculate the total number of duplicate records\n","  val duplicateCount = duplicateGroupsDF\n","    .select(expr(\"sum(count - 1)\")) // Subtract 1 for each group to exclude the first record\n","    .collect()(0)(0) // Extract the result from Row\n","\n","  // Step 4: Return the total count of duplicate records\n","  duplicateCount.toString.toLong\n","}"]},{"cell_type":"code","execution_count":7,"id":"59e6bb57","metadata":{"scrolled":false},"outputs":[{"data":{"text/plain":["1001"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["countDuplicates(moviesDF)"]},{"cell_type":"code","execution_count":8,"id":"6378b203","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["No duplicate groups found.\n"]},{"data":{"text/plain":["0"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["countDuplicates(cleanedMoviesDF)"]},{"cell_type":"code","execution_count":null,"id":"328c1eb8","metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Apache Toree - Scala","language":"scala","name":"apache_toree_scala"},"language_info":{"codemirror_mode":"text/x-scala","file_extension":".scala","mimetype":"text/x-scala","name":"scala","pygments_lexer":"scala","version":"2.12.15"}},"nbformat":4,"nbformat_minor":5}